
### Gradient Descent

To minimize the cost function \( J(\theta) \), gradient descent is commonly used. The parameters \(\theta\) are updated iteratively in the direction of the negative gradient of the cost function. The update rule is given by:

\[ \theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j} \]

where:
- \( \alpha \) is the learning rate.
- \( \frac{\partial J(\theta)}{\partial \theta_j} \) is the partial derivative of the cost function with respect to parameter \( \theta_j \).

For linear regression, the partial derivative of the cost function with respect to \(\theta_j\) is:

\[ \frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i) x_{ij} \]

Using this, the gradient descent update rule becomes:

\[ \theta_j := \theta_j - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i) x_{ij} \right) \]

By iteratively applying this update rule, the cost function \( J(\theta) \) is minimized, leading to optimal values for the parameters \(\theta\).


Let's consider a real-world example of using Gradient Descent in a linear regression model to predict house prices based on their size (square footage).

### Example Scenario
You have a dataset of house prices along with their sizes in square feet, and you want to build a model to predict the price of a house given its size.

### Dataset
Suppose you have the following data:
- House 1: Size = 1500 sq ft, Price = $300,000
- House 2: Size = 2000 sq ft, Price = $400,000
- House 3: Size = 2500 sq ft, Price = $500,000
- House 4: Size = 3000 sq ft, Price = $600,000

### Step-by-Step Application of Gradient Descent

1. **Initialization**
   - Start with initial guesses for the parameters: \(\theta_0 = 0\) (intercept) and \(\theta_1 = 0\) (slope).

2. **Hypothesis Function**
   - The hypothesis function for linear regression: \( h_\theta(x) = \theta_0 + \theta_1 x \)
   - For a house size (\( x \)), the predicted price (\( \hat{y} \)) is: \( \hat{y} = \theta_0 + \theta_1 x \)

3. **Cost Function**
   - Use the Mean Squared Error (MSE) as the cost function:
   \[ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i)^2 \]
   - Here, \( m \) is the number of training examples (houses), \( y_i \) is the actual price, and \( h_\theta(x_i) \) is the predicted price.

4. **Calculate the Gradient**
   - Compute the gradient of the cost function with respect to \(\theta_0\) and \(\theta_1\):
   \[ \frac{\partial J(\theta)}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i) \]
   \[ \frac{\partial J(\theta)}{\partial \theta_1} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i) x_i \]

5. **Update the Parameters**
   - Adjust the parameters using the learning rate \(\alpha\):
   \[ \theta_0 := \theta_0 - \alpha \frac{\partial J(\theta)}{\partial \theta_0} \]
   \[ \theta_1 := \theta_1 - \alpha \frac{\partial J(\theta)}{\partial \theta_1} \]

6. **Iterate Until Convergence**
   - Repeat steps 3-5 until the cost function converges (changes very little) or for a set number of iterations.

### Implementation in Python

```python
import numpy as np

# Dataset
X = np.array([1500, 2000, 2500, 3000])
y = np.array([300000, 400000, 500000, 600000])

# Parameters
theta_0 = 0
theta_1 = 0
alpha = 0.00000001  # Learning rate
num_iterations = 1000
m = len(y)  # Number of training examples

# Gradient Descent
for i in range(num_iterations):
    # Hypothesis function
    y_pred = theta_0 + theta_1 * X
    
    # Cost function (not necessary for gradient descent, but useful to track progress)
    cost = (1/(2*m)) * np.sum((y_pred - y)**2)
    
    # Calculate gradients
    d_theta_0 = (1/m) * np.sum(y_pred - y)
    d_theta_1 = (1/m) * np.sum((y_pred - y) * X)
    
    # Update parameters
    theta_0 = theta_0 - alpha * d_theta_0
    theta_1 = theta_1 - alpha * d_theta_1
    
    # Optionally, print the cost for each iteration to monitor convergence
    if i % 100 == 0:
        print(f"Iteration {i}: Cost {cost}, theta_0 {theta_0}, theta_1 {theta_1}")

# Final parameters
print(f"Final parameters: theta_0 = {theta_0}, theta_1 = {theta_1}")
```

### Results
After running the gradient descent algorithm, you will get values for \(\theta_0\) and \(\theta_1\) that best fit your data. These parameters can then be used to predict the price of a house given its size. For example, if \(\theta_0 = 50000\) and \(\theta_1 = 150\), the model predicts:

- For a house size of 1800 sq ft: \( \hat{y} = 50000 + 150 \cdot 1800 = \$320,000 \)

This example demonstrates how gradient descent can be used to find the optimal parameters for a linear regression model to predict house prices based on size.