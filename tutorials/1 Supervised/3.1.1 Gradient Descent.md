
### Gradient Descent

To minimize the cost function \( J(\theta) \), gradient descent is commonly used. The parameters \(\theta\) are updated iteratively in the direction of the negative gradient of the cost function. The update rule is given by:

\[ \theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j} \]

where:
- \( \alpha \) is the learning rate.
- \( \frac{\partial J(\theta)}{\partial \theta_j} \) is the partial derivative of the cost function with respect to parameter \( \theta_j \).

For linear regression, the partial derivative of the cost function with respect to \(\theta_j\) is:

\[ \frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i) x_{ij} \]

Using this, the gradient descent update rule becomes:

\[ \theta_j := \theta_j - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i) x_{ij} \right) \]

By iteratively applying this update rule, the cost function \( J(\theta) \) is minimized, leading to optimal values for the parameters \(\theta\).