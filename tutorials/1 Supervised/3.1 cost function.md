In the context of linear regression, the cost function, often referred to as the loss function, measures the performance of a model by quantifying the difference between the predicted values and the actual values. The most common cost function for linear regression is the Mean Squared Error (MSE). The formula for the cost function (J) in terms of MSE is given by:

\[ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i)^2 \]

where:
- \( J(\theta) \) is the cost function.
- \( m \) is the number of training examples.
- \( h_\theta(x_i) \) is the predicted value for the \( i \)-th training example (also known as the hypothesis).
- \( y_i \) is the actual value for the \( i \)-th training example.
- \( \theta \) represents the parameters (coefficients) of the model.

### Breakdown of the Formula

1. **Predicted Value (\( h_\theta(x_i) \))**: The hypothesis for a linear regression model is typically defined as:
   \[ h_\theta(x_i) = \theta_0 + \theta_1 x_{i1} + \theta_2 x_{i2} + \cdots + \theta_n x_{in} \]
   where \( x_{ij} \) are the features of the \( i \)-th example.

2. **Residual (Error)**: The difference between the predicted value and the actual value.
   \[ \text{Residual}_i = h_\theta(x_i) - y_i \]

3. **Squared Residual**: Squaring the residual to ensure positive values and to penalize larger errors more.
   \[ (\text{Residual}_i)^2 = (h_\theta(x_i) - y_i)^2 \]

4. **Sum of Squared Residuals**: Summing the squared residuals for all training examples.
   \[ \sum_{i=1}^{m} (h_\theta(x_i) - y_i)^2 \]

5. **Average Squared Residuals**: Dividing by the number of training examples to get the average squared residual.
   \[ \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i)^2 \]

6. **Scaling Factor**: The factor \(\frac{1}{2}\) is included for mathematical convenience, particularly when deriving the gradient for optimization algorithms (such as gradient descent).
   \[ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i)^2 \]
